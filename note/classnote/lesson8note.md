# 8w  无监督学习和降维

看了目录，这周内容不是一般地多……

## 1. Unsupervised Learning

### 1.1 Clustering

#### 1.1.1 Unsupervised Learning: Introduction  
3 分

看ng老师雀跃的表情和“保证精彩”……  

举例：  
无监督学习和监督学习的区别？  
没有y  

聚类这种无监督学习是怎样的效果？  
自动“聚”成不同类

![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/94342434.jpg)

#### 1.1.2 K-Means Algorithm  
13 分

主要问题：什么是**K-means算法**以及它是怎么运作的  

> 首先随机选择两（就是K，将要聚的类别数）个点，作为聚类中心 (cluster centroids)   
> K均值是一个迭代方法 它要做两件事情：第一个是簇分配，第二个是移动聚类中心。  
> 
> - 簇分配：遍历所有的样本，依据每一个点是更接近哪个中心将它分配到两个不同的聚类中心中；   
> - 移动聚类中心：将两个聚类中心，移动到和它一样颜色的那堆点的**均值**处。  
> 
> 循环以上簇分配和移动聚类中心的过程，直到点的簇不变，聚类中心不再移动为止。 

以上说K均值如何工作。  
 
以下说K均值的输入：  

> K均值算法接受两个输入：第一个是参数K，表示你想从数据中聚类出的簇的个数；另外一个输入是只有x没有标签y的训练集。  
> 
> 这里注意：在非监督学习的K均值算法里，我们约定 x(i) 是一个n维向量。这就是训练样本是 n 维而不是 n+1 维的原因。  

**训练集没有常数项。**  

异常情况（其实是实际中更普遍的情况）：  

>  原本看起来并没有三个分开的簇。但是从某种程度上讲，K均值仍然能将数据分成几个类。 

![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/79248835.jpg)

#### 1.1.3 Optimization Objective  
7 分

主要问题：K均值的优化目标函数（或者需要最小化的代价函数是什么？   

> 解决这个问题有两个好处：一是保证K均值正确运行，二是找到更好的簇，避免局部最优解。  

符号表示：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/49256823.jpg)    
> c表示序号，miu表示聚类中心。所以miu(c(i))表示的是xi所在的簇的聚类中心。  
> 
> K均值中，我们用大写K来表示簇的总数，用小写k来表示聚类中心的序号。因此，小写k的范围，就应该是1到大写K之间。 

最优化函数：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/56985366.jpg)   
> 
> 那么 K均值算法 要做的事情就是 它将找到参数 c(i) 和 μi 也就是说 找到能够最小化 代价函数 J 的 c 和 μ 这个代价函数 在K均值算法中 有时候也叫做 失真代价函数(distortion cost function)

点离聚类中心的距离的平方和。  

![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/54093315.jpg)  

因为最近，所以代价最低。 

练习：  
![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-9/30353624.jpg)  

这道题有点模糊。为什么会这样呢？

#### 1.1.4 Random Initialization  
8 分

主要问题：如何初始化  

> 通常用来初始化K均值聚类的方法是：随机挑选K个训练样本，然后设定μ1到μk让它们等于这个K个样本。  

这种方法带来的问题： 

> K均值方法最终可能会得到不同的结果， 尤其是如果K均值方法落在局部最优的时候。它取决于聚类簇的初始化方法，因此也就取决于随机的初始化。 

解决办法：  

> 如果你担心K均值方法会遇到局部最优的问题，如果 你想提高K均值方法找到最有可能的聚类的几率的话，我们能做的是尝试**多次随机**的初始化，初始化K均值很多次，并运行K均值方法很多次，通过多次尝试来保证我们最终能得到一个足够好的结果，一个尽可能局部或全局最优的结果。  
 
适用情况：  

> **聚类数相对较小**的体系里，特别是如果你有2个或者3个或者4个聚类的话，随机初始化会有较大的影响，可以保证你在最小化失真函数的时候得到一个很小的值 并且能得到一个很好的聚类结果。  

#### 1.1.5 Choosing the Number of Clusters  
8 分

主要问题：如何去选择参数大写K的值？  

问题的难点在于“数据集中有多少个聚类通常是模棱两可的”。

> 1. 最为常见的方法，实际上仍是**手动选择**聚类的数目。  

> 2. 肘部法则 (Elbow Method)：改变K（类别总数）的值，用K值为1逐步增加，来运行K-均值聚类算法，然后计算代价函数，画出来的曲线展示了随着聚类数量的增多，畸变值J是逐步下降的。 
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/20608655.jpg)  
> 选择下降开始缓慢的“肘部”点（像人的肘部而得名）为聚类数目。  
> 
> 3. 通常来说，决定聚类数量的更好的办法是：看不同的聚类数量能为后续下游的目的提供多好的结果。

## 2. Dimensionality Reduction

第二种无监督学习问题 它叫维数约减 (dimensionality reduction)   

### 2.1 Motivation

#### 2.1.1 Motivation I: Data Compression  
10 分  

降维的好处：数据压缩

dimensionality reduction可以进行数据压缩，数据压缩的好处：  
- 数据占据更小的计算机内存和空间；  
- 算法运行**更快**。（这一点更重要）

把原始样本进行适当的映射，降低维度，减少数据冗余。

#### 2.1.2 Motivation II: Visualization  
5 分

降维的好处：数据可视化更方便

降维后再数据可视化  

降维前：  
![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/2998027.jpg)  

降维后：  
![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/46100230.jpg)  

降维后可视化结果：  
![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/92723170.jpg)

### 2.2 Principal Component Analysis

这就是“主成分分析”。 

> 对于降维问题来说 目前 最流行 最常用的算法是 主成分分析法 (Principal Componet Analysis, PCA）。 

#### 2.2.1 Principal Component Analysis Problem Formulation  
9 分

在低维上找到一个（或一组）向量，将原来的数据点投影上去，能令投影误差最小。这就实现了降维。

![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/96573151.jpg)

PCA和线性回归的区别：  

![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/79688793.jpg)  

注意蓝色线条的方向！！！  

> 理解PCA 是做什么的:它是寻找到一个低维的平面，对数据进行投影，以便最小化投影误差的平方，最小化每个点，与投影后的对应点之间的距离的平方值。

#### 2.2.2 Principal Component Analysis Algorithm  
15 分

主要问题：PCA的算法  

> 数据预处理：拿到某组有m个无标签样本的训练集，一般先进行均值归一化 (mean normalization) 。或者feature scaling，这根据你的数据而定。   
 
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/42012258.jpg)
> 
> 归一化即与平均值的差，再除以适当的单位则是feature scaling。 
 
要把数据从n维降低到k维： 

> 首先，计算出这个协方差矩阵（通常是用希腊字母大写的西格玛∑来表示）  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/6120725.jpg)  
> 计算出这个协方差矩阵后，假如我们把它存为 Octave中的变量Sigma，我们需要计算出Sigma矩阵的特征向量(eigenvectors)：   
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/34119143.jpg)  
> （ `svd`表示奇异值分解 (singular value decomposition)  ）  
> （Sigma是一个协方差矩阵，有很多种方法来计算它的特征向量。在Octave中，还有另一个`eig`命令可以用来计算特征向量。两个函数不同，计算协方差矩阵的特征向量时结果相同。因为协方差均值总满足一个数学性质，称为对称正定 (symmetric positive definite)。）  
> （这个协方差矩阵Sigma应该是一个n×n的矩阵。）  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/242604.jpg)  
> 如果我们想将数据的维度从n降低到k的话，我们只需要提取前 k 列向量，得到了 u(1) 到 u(k) 也就是我们用来投影数据的k个方向。  
> 通过这个svd，我们得到了矩阵U我们把它的列向量叫做u(1)到 u(n)。  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/55548078.jpg)  
> 通过这个svd 过程，我们可以得到矩阵U S V。我们取出U矩阵的前k列，得到一个新的矩阵u(1)到u(k)。因此这就是一个 n × k 维的矩阵 n × k 维的矩阵 我给这个矩阵起个名字 我把这个矩阵 叫做 Ureduce 表示 U 矩阵约减后的版本。   
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/8184359.jpg)
> 然后 计算 z 的方法是：z 等于这个 Ureduce的转置（它的维度是k×n）乘以x，x 的维度是 n × 1，因此这两个相乘 维度应该是 k × 1 因此 z 是 k 维的 向量。  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/479508.jpg)  

总结：  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/80002999.jpg)   

注意：  
- 如果不同特征量的范围跨度很大的话，需要进行特征缩放这一步。  
- 使用PCA时，x应该是n维实数，所以没有x0 = 1这一项。

这个题目是个陷阱：  
![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/48882663.jpg)  

z的第j个值，实际上是个实数。起初选择了A，那是标准公式下的向量。  

### 2.3 Applying PCA

#### 2.3.1 Reconstruction from Compressed Representation  
4 分

主要问题：从压缩过的数据近似地回到原始高维度的数据。  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/38233301.jpg)  

题目有意思，目的是啥呢？  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/30789354.jpg)  

感觉这个思维实验没啥意义。这不都明摆着的吗……

#### 2.3.2 Choosing the Number of Principal Components  
11 分

主要问题：如何选择 PCA 的参数k？  

PCA所做的是**尽量最小化**。   

平均平方映射误差 (Average Squared Projection Error)  

> 原始数据x和映射值x_approx(i)之间的差，最小化 x和其在低维表面上的映射点之间的距离的平方。  
 
数据的总变差 (Total Variation)  

> 是这些样本x(i)的长度的平方的均值，它的意思是 “平均来看，我的训练样本距离零向量多远？距离原点多远？”   

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/90239943.jpg)

如何选择k值？  

> 平均平方映射误差/数据的总变差   
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/87063471.jpg)
> 想要这个比值能够小于（比如说）0.01（或者1%）   

> 考虑k值选择时，考虑的不是k是多少，而是类似0.01这个标准是多少。如果是0.01，用PCA的语言说就是“保留了99%的差异性”，即：使得99%的差异性得以保留。

常见选择：  

- 0.01  
- 0.05  
- 0.10

如何实现？  

方法一：  
从k＝1开始计算，直到某个k值令比值等于0.01为止。  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/19440409.jpg)

比较低效。  

方法二：  
回到USV矩阵，利用其中的S矩阵。  

> S矩阵也是n＊n维度。它是一个对角矩阵。 
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/81620837.jpg)  
 
看这个图计算太爽了。变成这样：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/9756533.jpg)  

总结就是这样：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/92658423.jpg)   

（谁发现的这个，功德无量啊）

#### 2.3.3 Advice for Applying PCA  
13 分

主要内容：如何在实际中操作？以及一些PCA应用建议。  

如何通过 PCA 来提高学习算法的速度？ 

一图可知：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/60245889.jpg)    

注意：  

> PCA定义了从x到z的对应关系，这种从 x 到 z的对应关系只可以通过在训练集上运行PCA定义出来。这种 PCA所学习出的对应关系，所做的就是计算出一系列的参数（这就是特征缩放和均值归一化），同时也计算出这样一个降维的矩阵Ureduce。  
> 
> 但是降维矩阵Ureduce中的数据，就像一个PCA所学习的参数一样，唯一地适应这些训练集，而不是适应我们的交叉验证或者测试集。因此Ureduce矩阵中的数据 就应该只通过对训练集运行PCA来获得。 
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/49918367.jpg)  
> 
> 在 运行PCA的时候 只是在训练集那一部分来进行的 而不是交叉验证的数据集，这就定义了从 x到z的映射。然后你就可以将这个映射应用到交叉验证数据集中和测试数据集中。  

感觉他说的意思是：在训练集PCA就得到训练集的主成分，在交叉验证集PCA就得到交叉验证集的主成分，在测试集PCA就得到测试集的主成分。  

是不是这个意思呢？？  

总结PCA的应用：  

> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/31770495.jpg)  

经常被误用的PCA场景：使用PCA来避免过拟合。  
> ![](http://7xotr7.com1.z0.glb.clouddn.com/16-5-10/66311105.jpg)  

PCA不需要y，而避免过拟合需要y，所以避免过拟合最好的方法是正则化（lambda）。  

为了避免误用，要考虑：  

> **如果不使用PCA会怎样？**  
> 在使用PCA之前，让我们就试试在学习算法上使用原始数据x(i)看看效果。  
> 
> 只有一个原因，让我们相信算法出现了问题，那就是 你的学习算法“收敛地非常缓慢、占用内存或者硬盘空间非常大”，所以你想来压缩数据。  
> 
> 只有当你的x(i)效果不好，只有当你有证据或者充足的理由来确定 x(i)效果不好的时候，才考虑用PCA来进行压缩数据。 

